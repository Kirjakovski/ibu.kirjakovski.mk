---
title: "LECTURE 8: Correlation and Regression"
date: 2025-12-22T16:00:00+02:00
deadline:
categories: [Advanced Quantitative Research Methods]
draft: false
---
#### Summary:

Correlation and regression are fundamental statistical procedures used to analyze relationships between numeric, equal-interval variables. Correlation describes the degree of association between variables, while regression utilizes these associations to facilitate precise outcome predictions. Scatter diagrams provide a visual representation of these patterns, indicating whether a relationship is linear, curvilinear, or absent. Linear correlations are defined by their direction—positive or negative—and their strength, which ranges from weak to perfect.

The Pearson correlation coefficient ($r$) serves as both a descriptive and inferential statistic, ranging from -1 to +1. Its logic is rooted in Z-score standardization and the average of cross-products, where positive sums indicate positive linear associations and sums near zero suggest no relationship. Significance testing determines if a sample correlation is likely present in the general population, typically through a t-test. Key assumptions for these procedures include normality, bivariate normality, homoscedasticity, and the independence of scores.

Statistical prediction designates one variable as the predictor ($X$) and another as the criterion variable ($Y$). The linear prediction rule, $\hat{Y}=a+(b)(X)$, establishes a regression line where the constant ($a$) represents the intercept and the coefficient ($b$) determines the slope. This model adheres to the least squares criterion, which minimizes the sum of squared errors between predicted and actual values. Standardized regression coefficients ($\beta$) allow for comparison across different measurement scales by expressing change in standard deviation units.

Multiple regression expands this framework by incorporating two or more predictors to enhance accuracy, measured by the multiple correlation coefficient ($R$). Despite their predictive utility, these procedures have inherent limitations. Accuracy is compromised by curvilinear relationships, restricted data ranges, unreliable measurements, and the presence of outliers. Crucially, neither correlation nor regression establishes causality, as these methods do not inherently determine temporal order or eliminate the possibility of third-variable influences.